{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "World\n",
      "Font\n",
      "Line\n",
      "Moon\n",
      "Astronomical object\n",
      "Illustration\n",
      "Circle\n",
      "Electric blue\n",
      "Art\n",
      "Graphics\n",
      "Texts:\n",
      "**\n",
      "산책 소모임\n",
      "해질 무렵\n",
      "밤에\n",
      "w\n",
      "LOOD\n",
      "w\n",
      "c\n",
      "JJ\n",
      "m\n",
      "**\n",
      "산책\n",
      "소모임\n",
      "해질\n",
      "무렵\n",
      "밤\n",
      "에\n",
      "w\n",
      "LOOD\n",
      "w\n",
      "c\n",
      "JJ\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"ocr-project-386007-133b41eb4439.json\"\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "\n",
    "# Instantiates a client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# The name of the image file to annotate\n",
    "file_name = os.path.abspath('poster.jpg')\n",
    "\n",
    "# Loads the image into memory\n",
    "with io.open(file_name, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.Image(content=content)\n",
    "\n",
    "# Performs label detection on the image file\n",
    "response = client.label_detection(image=image)\n",
    "labels = response.label_annotations\n",
    "\n",
    "print('Labels:')\n",
    "for label in labels:\n",
    "    print(label.description)\n",
    "    \n",
    "# Performs text detection on the image file\n",
    "response = client.text_detection(image=image)\n",
    "texts = response.text_annotations\n",
    "\n",
    "print('Texts:')\n",
    "for text in texts:\n",
    "    print(text.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 검출하고 바운딩 박스 그린 이미지 저장\n",
    "import io\n",
    "import os\n",
    "\n",
    "from google.cloud import vision_v1\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"ocr-project-386007-133b41eb4439.json\"\n",
    "\n",
    "client = vision_v1.ImageAnnotatorClient()\n",
    "img_path = 'poster'\n",
    "with io.open(img_path+'.jpg', \"rb\") as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision_v1.types.Image(content=content)\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "texts = response.text_annotations[1:]\n",
    "\n",
    "im = Image.open(io.BytesIO(content))\n",
    "\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for text in texts:\n",
    "    vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "    draw.polygon(vertices, outline='red')\n",
    "\n",
    "im.save(f\"result_{img_path}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[description: \"풀문\"\n",
       " bounding_poly {\n",
       "   vertices {\n",
       "     x: 61\n",
       "     y: 80\n",
       "   }\n",
       "   vertices {\n",
       "     x: 183\n",
       "     y: 81\n",
       "   }\n",
       "   vertices {\n",
       "     x: 183\n",
       "     y: 135\n",
       "   }\n",
       "   vertices {\n",
       "     x: 61\n",
       "     y: 134\n",
       "   }\n",
       " },\n",
       " description: \"스튜디오\"\n",
       " bounding_poly {\n",
       "   vertices {\n",
       "     x: 188\n",
       "     y: 80\n",
       "   }\n",
       "   vertices {\n",
       "     x: 435\n",
       "     y: 81\n",
       "   }\n",
       "   vertices {\n",
       "     x: 435\n",
       "     y: 136\n",
       "   }\n",
       "   vertices {\n",
       "     x: 188\n",
       "     y: 135\n",
       "   }\n",
       " },\n",
       " description: \"MARA\"\n",
       " bounding_poly {\n",
       "   vertices {\n",
       "     x: 388\n",
       "     y: 442\n",
       "   }\n",
       "   vertices {\n",
       "     x: 410\n",
       "     y: 442\n",
       "   }\n",
       "   vertices {\n",
       "     x: 410\n",
       "     y: 448\n",
       "   }\n",
       "   vertices {\n",
       "     x: 388\n",
       "     y: 448\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     content \u001b[39m=\u001b[39m image_file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     16\u001b[0m image \u001b[39m=\u001b[39m vision_v1\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mImage(content\u001b[39m=\u001b[39mcontent)\n\u001b[1;32m---> 18\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mtext_detection(image\u001b[39m=\u001b[39;49mimage)\n\u001b[0;32m     19\u001b[0m texts \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext_annotations[\u001b[39m1\u001b[39m:]\n\u001b[0;32m     21\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(io\u001b[39m.\u001b[39mBytesIO(content))\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\google\\cloud\\vision_helpers\\decorators.py:112\u001b[0m, in \u001b[0;36m_create_single_feature_method.<locals>.inner\u001b[1;34m(self, image, max_results, retry, timeout, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     copied_features[\u001b[39m\"\u001b[39m\u001b[39mmax_results\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m max_results\n\u001b[0;32m    111\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(image\u001b[39m=\u001b[39mimage, features\u001b[39m=\u001b[39m[copied_features], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 112\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mannotate_image(\n\u001b[0;32m    113\u001b[0m     request, retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout, metadata\u001b[39m=\u001b[39;49mmetadata\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\google\\cloud\\vision_helpers\\__init__.py:76\u001b[0m, in \u001b[0;36mVisionHelpers.annotate_image\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(request\u001b[39m.\u001b[39mfeatures) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     75\u001b[0m     request\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_all_features()\n\u001b[1;32m---> 76\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_annotate_images(\n\u001b[0;32m     77\u001b[0m     requests\u001b[39m=\u001b[39;49m[request], retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout, metadata\u001b[39m=\u001b[39;49mmetadata\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     79\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mresponses[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\google\\cloud\\vision_v1\\services\\image_annotator\\client.py:567\u001b[0m, in \u001b[0;36mImageAnnotatorClient.batch_annotate_images\u001b[1;34m(self, request, requests, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    564\u001b[0m rpc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39m_wrapped_methods[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39mbatch_annotate_images]\n\u001b[0;32m    566\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[0;32m    568\u001b[0m     request,\n\u001b[0;32m    569\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[0;32m    570\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    571\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m    572\u001b[0m )\n\u001b[0;32m    574\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     metadata\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata)\n\u001b[0;32m    111\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metadata\n\u001b[1;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(callable_)\n\u001b[0;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_remapped_callable\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     71\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m         \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     73\u001b[0m     \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     74\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\grpc\\_channel.py:1028\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m   1022\u001b[0m              request: Any,\n\u001b[0;32m   1023\u001b[0m              timeout: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m              wait_for_ready: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1027\u001b[0m              compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m-> 1028\u001b[0m     state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                                   wait_for_ready, compression)\n\u001b[0;32m   1030\u001b[0m     \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\grpc\\_channel.py:1017\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1010\u001b[0m     call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_channel\u001b[39m.\u001b[39msegregated_call(\n\u001b[0;32m   1011\u001b[0m         cygrpc\u001b[39m.\u001b[39mPropagationConstants\u001b[39m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1012\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_method, \u001b[39mNone\u001b[39;00m, _determine_deadline(deadline), metadata,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1016\u001b[0m         ),), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context)\n\u001b[1;32m-> 1017\u001b[0m     event \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49mnext_event()\n\u001b[0;32m   1018\u001b[0m     _handle_event(event, state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1019\u001b[0m     \u001b[39mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:338\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:169\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/channel.pyx.pxi:163\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\python\\grpcio\\grpc\\_cython\\_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 바운딩 박스로 검출된 부분 crop 해서 저장\n",
    "import io\n",
    "import os\n",
    "\n",
    "from google.cloud import vision_v1\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"ocr-project-386007-133b41eb4439.json\"\n",
    "\n",
    "client = vision_v1.ImageAnnotatorClient()\n",
    "img_path = 'poster'\n",
    "\n",
    "with io.open(img_path+\".jpg\", \"rb\") as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision_v1.types.Image(content=content)\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "texts = response.text_annotations[1:]\n",
    "\n",
    "im = Image.open(io.BytesIO(content))\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "    left = min(vertices, key=lambda x: x[0])[0]\n",
    "    upper = min(vertices, key=lambda x: x[1])[1]\n",
    "    right = max(vertices, key=lambda x: x[0])[0]\n",
    "    lower = max(vertices, key=lambda x: x[1])[1]\n",
    "    im_crop = im.crop((left, upper, right, lower))\n",
    "    im_crop.save(f\"result_{img_path}_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바운딩 박스 부분 crop하고 글자는 검정 배경은 흰색으로 바꾸기(잘안됨)\n",
    "import io\n",
    "import os\n",
    "\n",
    "from google.cloud import vision_v1\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"ocr-project-386007-133b41eb4439.json\"\n",
    "\n",
    "client = vision_v1.ImageAnnotatorClient()\n",
    "\n",
    "with io.open(\"풀문스튜디오.jpg\", \"rb\") as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision_v1.types.Image(content=content)\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "texts = response.text_annotations[1:]\n",
    "\n",
    "im = Image.open(io.BytesIO(content))\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "    vertices.sort(key=lambda x: x[0])\n",
    "    left = vertices[0][0]\n",
    "    right = vertices[-1][0]\n",
    "    vertices.sort(key=lambda x: x[1])\n",
    "    top = vertices[0][1]\n",
    "    bottom = vertices[-1][1]\n",
    "\n",
    "    # crop the bounding box area\n",
    "    im_crop = im.crop((left, top, right, bottom))\n",
    "\n",
    "    # get character region\n",
    "    im_gray = im_crop\n",
    "    im_bin = im_gray.point(lambda x: 0 if x > 128 else 255)\n",
    "    region = im_bin.crop(im_bin.getbbox())\n",
    "\n",
    "    # save character region\n",
    "    region.save(f\"result_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_path = r'studio.jpg'\n",
    "\n",
    "# 이미지 로드\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# 그레이 스케일로 변환\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 이진화\n",
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# 경계선 검출\n",
    "edges = cv2.Canny(thresh, 100, 200)\n",
    "\n",
    "# 결과 출력\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Inverted Image', thresh)\n",
    "cv2.imshow('Detected Edges', edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 750, 3)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('cafe.jpg')\n",
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
